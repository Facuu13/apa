{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Predicción de tormenta en base de Datos Meteorológicos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrantes\n",
    "\n",
    "- Andrioli, Facundo\n",
    "- Pérez, José"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lea el dataset limpio que se guardó en el TP1 (punto 2 de conclusiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargamos el conjunto de datos\n",
    "file = 'weatherAUS_output.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Realice una codificación adecuada de variables categóricas si es necesario, utilizando técnicas como One-Hot \n",
    "Encoding según corresponda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
      "0     12.7     15.8       0.8          1.4       7.8           35.0   \n",
      "1      6.2     15.1       0.0          1.8       2.1           20.0   \n",
      "2      5.3     15.9       0.0          1.4       8.0           30.0   \n",
      "3     11.3     15.7       0.0          1.4       1.5           52.0   \n",
      "4      7.6     11.2      16.2          4.6       1.1           46.0   \n",
      "\n",
      "   WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  ...  WindDir3pm_NNW  \\\n",
      "0          13.0          15.0         75.0         52.0  ...           False   \n",
      "1           2.0          11.0         81.0         56.0  ...           False   \n",
      "2           6.0          13.0         71.0         46.0  ...           False   \n",
      "3          15.0          22.0         62.0         62.0  ...            True   \n",
      "4          17.0          13.0         83.0         88.0  ...           False   \n",
      "\n",
      "   WindDir3pm_NW  WindDir3pm_S  WindDir3pm_SE  WindDir3pm_SSE  WindDir3pm_SSW  \\\n",
      "0          False         False          False           False           False   \n",
      "1          False         False          False           False           False   \n",
      "2          False         False          False           False           False   \n",
      "3          False         False          False           False           False   \n",
      "4          False         False          False           False           False   \n",
      "\n",
      "   WindDir3pm_SW  WindDir3pm_W  WindDir3pm_WNW  WindDir3pm_WSW  \n",
      "0           True         False           False           False  \n",
      "1           True         False           False           False  \n",
      "2          False         False           False           False  \n",
      "3          False         False           False           False  \n",
      "4           True         False           False           False  \n",
      "\n",
      "[5 rows x 3547 columns]\n"
     ]
    }
   ],
   "source": [
    "# Identificamos las columnas categóricas\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Aplicamos One-Hot Encoding a las columnas categóricas\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Mostramos las primeras filas del dataset transformado\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Separe el dataset en entrenamiento y prueba (70% entrenamiento, 30% prueba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de X_train: 352834092\n",
      "Valores de y_train: 99502\n",
      "Valores de X_test: 42644\n",
      "Valores de y_test: 42644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definimos las características (X) y el objetivo (y)\n",
    "X = df_encoded.drop('RainTomorrow', axis=1)\n",
    "y = df_encoded['RainTomorrow']\n",
    "\n",
    "# Separamos el dataset en entrenamiento (70%) y prueba (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Datos de train y de test\n",
    "print(\"Valores de X_train:\",X_train.size)\n",
    "print(\"Valores de y_train:\",y_train.size)\n",
    "print(\"Valores de X_test:\",len(X_test))\n",
    "print(\"Valores de y_test:\",len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Normalice o estandarice los atributos numéricos para asegurar que todas las variables tengan la misma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MinTemp   MaxTemp  Rainfall  Evaporation  Sunshine  WindGustSpeed  \\\n",
      "5426  -0.728788 -1.024421 -0.136110    -0.529909 -0.727728      -1.583889   \n",
      "75590  1.160302  0.576340 -0.277473    -0.529909 -1.276662       0.281685   \n",
      "9368   0.098664  1.250345 -0.277473     0.171888  1.111201       0.558067   \n",
      "73772 -0.822462 -0.813794 -0.277473    -0.529909 -1.276662       0.212590   \n",
      "3398   0.286012 -0.855920 -0.230352    -0.572442 -1.249215      -1.583889   \n",
      "\n",
      "       WindSpeed9am  WindSpeed3pm  Humidity9am  Humidity3pm  ...  \\\n",
      "5426      -1.572602      2.009055     1.458635     0.935639  ...   \n",
      "75590      0.113636      0.794151     0.783584     1.270228  ...   \n",
      "9368      -0.336027      0.794151    -2.955154    -2.123465  ...   \n",
      "73772      0.563299      0.131476    -0.203027    -0.880704  ...   \n",
      "3398       1.575042      1.015042     0.523950     1.557019  ...   \n",
      "\n",
      "       WindDir3pm_NNW  WindDir3pm_NW  WindDir3pm_S  WindDir3pm_SE  \\\n",
      "5426            False          False         False           True   \n",
      "75590           False          False         False          False   \n",
      "9368            False           True         False          False   \n",
      "73772           False          False         False          False   \n",
      "3398            False          False         False          False   \n",
      "\n",
      "       WindDir3pm_SSE  WindDir3pm_SSW  WindDir3pm_SW  WindDir3pm_W  \\\n",
      "5426            False           False          False         False   \n",
      "75590           False           False          False         False   \n",
      "9368            False           False          False         False   \n",
      "73772            True           False          False         False   \n",
      "3398            False           False          False         False   \n",
      "\n",
      "       WindDir3pm_WNW  WindDir3pm_WSW  \n",
      "5426            False           False  \n",
      "75590           False           False  \n",
      "9368            False           False  \n",
      "73772           False           False  \n",
      "3398            False           False  \n",
      "\n",
      "[5 rows x 3546 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identificamos las columnas numéricas\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Creamos una instancia del StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustamos el escalador a los datos de entrenamiento y transformamos los datos de entrenamiento\n",
    "X_train[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "\n",
    "# Utilizamos el mismo escalador para transformar los datos de prueba\n",
    "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "# Mostramos las primeras filas del conjunto de entrenamiento transformado\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado y Evaluación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine dos métricas de evaluación que considere importante medir para este problema. Justifique su elección."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy (Exactitud)\n",
    "\n",
    "- Mide la proporción de instancias correctamente clasificadas en comparación con el total de instancias. Es una métrica fácil de interpretar y adecuada cuando las clases están relativamente balanceadas.\n",
    "\n",
    "- Te proporciona una visión general del rendimiento del modelo, ya que muestra qué porcentaje de las predicciones fueron correctas.\n",
    "\n",
    "#### F1-score\n",
    "\n",
    "- F1-score es la media armónica de la precisión y el recall, y es especialmente útil en casos donde las clases están desbalanceadas.\n",
    "\n",
    "- Es una métrica robusta que toma en cuenta tanto los falsos positivos como los falsos negativos, y es útil cuando es importante minimizar ambos tipos de errores.\n",
    "\n",
    "- El F1-score te da una mejor idea del equilibrio entre precisión y recall, especialmente si una clase es más importante que otra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implemente un modelo de base usando como predicción que determine que llueve mañana si hoy llueve, y si hoy no llueve\n",
    "mañana no va a llover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo base: 0.7605759309633243\n",
      "Matriz de confusión:\n",
      "[[27997  5141]\n",
      " [ 5069  4437]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def base_model_prediction(X):\n",
    "  return X['RainToday'].astype(int)\n",
    "\n",
    "# Predecir con el modelo de base\n",
    "y_pred_base = base_model_prediction(X_test) \n",
    "\n",
    "# Evaluar el modelo de base\n",
    "accuracy_base = accuracy_score(y_test, y_pred_base)\n",
    "conf_matrix_base = confusion_matrix(y_test, y_pred_base)\n",
    "\n",
    "print(f\"Exactitud del modelo base: {accuracy_base}\")\n",
    "print(\"Matriz de confusión:\")\n",
    "print(conf_matrix_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implemente y entrene un modelo de k Nearest Neighbors (kNN) para predecir si va a llover mañana. Utilice la \n",
    "distancia euclidiana como medida de distancia entre vecinos.Ajuste el parámetro k utilizando técnicas como la \n",
    "validación cruzada (5 folds) para optimizar el rendimiento del modelo (utilice como función de optimización una de las \n",
    "métricas definidas en el punto anterior). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definiendo el clasificador kNN...\n",
      "Iniciando GridSearchCV para encontrar el mejor valor de k...\n",
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n",
      "[CV 1/5] END .....................n_neighbors=1;, score=0.804 total time= 4.0min\n",
      "[CV 2/5] END .....................n_neighbors=1;, score=0.806 total time= 4.1min\n",
      "[CV 3/5] END .....................n_neighbors=1;, score=0.802 total time= 4.2min\n",
      "[CV 4/5] END .....................n_neighbors=1;, score=0.801 total time= 4.1min\n",
      "[CV 5/5] END .....................n_neighbors=1;, score=0.802 total time= 4.1min\n",
      "[CV 1/5] END .....................n_neighbors=3;, score=0.826 total time= 4.2min\n",
      "[CV 2/5] END .....................n_neighbors=3;, score=0.832 total time= 4.2min\n",
      "[CV 3/5] END .....................n_neighbors=3;, score=0.829 total time= 4.1min\n",
      "[CV 4/5] END .....................n_neighbors=3;, score=0.828 total time= 4.2min\n",
      "[CV 5/5] END .....................n_neighbors=3;, score=0.826 total time= 4.2min\n",
      "[CV 1/5] END .....................n_neighbors=5;, score=0.835 total time= 4.1min\n",
      "[CV 2/5] END .....................n_neighbors=5;, score=0.841 total time= 4.1min\n",
      "[CV 3/5] END .....................n_neighbors=5;, score=0.839 total time= 4.2min\n",
      "[CV 4/5] END .....................n_neighbors=5;, score=0.837 total time= 4.1min\n",
      "[CV 5/5] END .....................n_neighbors=5;, score=0.833 total time= 4.1min\n",
      "[CV 1/5] END .....................n_neighbors=7;, score=0.838 total time= 4.2min\n",
      "[CV 2/5] END .....................n_neighbors=7;, score=0.842 total time= 4.2min\n",
      "[CV 3/5] END .....................n_neighbors=7;, score=0.843 total time= 4.2min\n",
      "[CV 4/5] END .....................n_neighbors=7;, score=0.840 total time= 4.2min\n",
      "[CV 5/5] END .....................n_neighbors=7;, score=0.838 total time= 4.1min\n",
      "[CV 1/5] END .....................n_neighbors=9;, score=0.840 total time= 4.2min\n",
      "[CV 2/5] END .....................n_neighbors=9;, score=0.845 total time= 4.2min\n",
      "[CV 3/5] END .....................n_neighbors=9;, score=0.843 total time= 4.1min\n",
      "[CV 4/5] END .....................n_neighbors=9;, score=0.843 total time= 4.2min\n",
      "[CV 5/5] END .....................n_neighbors=9;, score=0.840 total time= 3.8min\n",
      "[CV 1/5] END ....................n_neighbors=11;, score=0.839 total time= 2.0min\n",
      "[CV 2/5] END ....................n_neighbors=11;, score=0.847 total time= 1.6min\n",
      "[CV 3/5] END ....................n_neighbors=11;, score=0.844 total time= 1.7min\n",
      "[CV 4/5] END ....................n_neighbors=11;, score=0.845 total time= 1.7min\n",
      "[CV 5/5] END ....................n_neighbors=11;, score=0.841 total time= 1.7min\n",
      "[CV 1/5] END ....................n_neighbors=13;, score=0.841 total time= 1.7min\n",
      "[CV 2/5] END ....................n_neighbors=13;, score=0.847 total time= 1.7min\n",
      "[CV 3/5] END ....................n_neighbors=13;, score=0.844 total time= 1.9min\n",
      "[CV 4/5] END ....................n_neighbors=13;, score=0.846 total time= 1.9min\n",
      "[CV 5/5] END ....................n_neighbors=13;, score=0.843 total time= 1.9min\n",
      "[CV 1/5] END ....................n_neighbors=15;, score=0.842 total time= 1.9min\n",
      "[CV 2/5] END ....................n_neighbors=15;, score=0.846 total time= 1.9min\n",
      "[CV 3/5] END ....................n_neighbors=15;, score=0.845 total time= 2.0min\n",
      "[CV 4/5] END ....................n_neighbors=15;, score=0.848 total time= 2.0min\n",
      "[CV 5/5] END ....................n_neighbors=15;, score=0.843 total time= 2.0min\n",
      "[CV 1/5] END ....................n_neighbors=17;, score=0.842 total time= 1.9min\n",
      "[CV 2/5] END ....................n_neighbors=17;, score=0.846 total time= 2.0min\n",
      "[CV 3/5] END ....................n_neighbors=17;, score=0.846 total time= 2.0min\n",
      "[CV 4/5] END ....................n_neighbors=17;, score=0.848 total time= 2.0min\n",
      "[CV 5/5] END ....................n_neighbors=17;, score=0.842 total time= 2.0min\n",
      "[CV 1/5] END ....................n_neighbors=19;, score=0.844 total time= 2.0min\n",
      "[CV 2/5] END ....................n_neighbors=19;, score=0.846 total time= 2.0min\n",
      "[CV 3/5] END ....................n_neighbors=19;, score=0.846 total time= 2.0min\n",
      "[CV 4/5] END ....................n_neighbors=19;, score=0.847 total time= 2.0min\n",
      "[CV 5/5] END ....................n_neighbors=19;, score=0.843 total time= 2.2min\n",
      "[CV 1/5] END ....................n_neighbors=21;, score=0.844 total time= 2.1min\n",
      "[CV 2/5] END ....................n_neighbors=21;, score=0.847 total time= 2.1min\n",
      "[CV 3/5] END ....................n_neighbors=21;, score=0.846 total time= 2.0min\n",
      "[CV 4/5] END ....................n_neighbors=21;, score=0.847 total time= 2.0min\n",
      "[CV 5/5] END ....................n_neighbors=21;, score=0.844 total time= 2.0min\n",
      "\n",
      "Mejor valor de k encontrado: 21\n",
      "Puntaje de validación cruzada (accuracy) con el mejor k: 0.85\n",
      "Entrenando el modelo kNN con el mejor valor de k...\n",
      "Modelo entrenado.\n",
      "Haciendo predicciones en el conjunto de prueba...\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.96      0.91     33138\n",
      "         1.0       0.76      0.45      0.57      9506\n",
      "\n",
      "    accuracy                           0.85     42644\n",
      "   macro avg       0.81      0.71      0.74     42644\n",
      "weighted avg       0.84      0.85      0.83     42644\n",
      "\n",
      "Exactitud del modelo: 0.846004127192571\n",
      "Matriz de confusión del modelo:\n",
      "[[31770  1368]\n",
      " [ 5199  4307]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Creamos un imputador que reemplace los NaN por la media de cada columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Aplicamos el imputador a los datos de entrenamiento y prueba\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "print(\"Definiendo el clasificador kNN...\")\n",
    "knn = KNeighborsClassifier(metric='euclidean')\n",
    "\n",
    "# Definimos los parámetros para GridSearchCV\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]}  # Probar valores de k impares\n",
    "print(\"Iniciando GridSearchCV para encontrar el mejor valor de k...\")\n",
    "\n",
    "# Usamos GridSearchCV para encontrar el mejor k usando validación cruzada (5 folds)\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', verbose=3)\n",
    "grid_search.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Mostramos el mejor parámetro k y el mejor puntaje\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_score = grid_search.best_score_\n",
    "print(f\"\\nMejor valor de k encontrado: {best_k}\")\n",
    "print(f\"Puntaje de validación cruzada (accuracy) con el mejor k: {best_score:.2f}\")\n",
    "\n",
    "# Entrenamos el modelo kNN con el mejor valor de k en el conjunto de entrenamiento\n",
    "print(\"Entrenando el modelo kNN con el mejor valor de k...\")\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k, metric='euclidean')\n",
    "best_knn.fit(X_train_imputed, y_train)\n",
    "print(\"Modelo entrenado.\")\n",
    "\n",
    "# Hacemos predicciones en el conjunto de prueba\n",
    "print(\"Haciendo predicciones en el conjunto de prueba...\")\n",
    "y_pred = best_knn.predict(X_test_imputed)\n",
    "\n",
    "# Evaluar el modelo\n",
    "class_report_knn = classification_report(y_test, y_pred)\n",
    "accuracy_knn = accuracy_score(y_test, y_pred)\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(\"Reporte de Clasificación:\")\n",
    "print(class_report_knn)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f\"Exactitud del modelo: {accuracy_knn}\")\n",
    "print(\"Matriz de confusión del modelo:\")\n",
    "print(conf_matrix_knn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Implemente y entrene un modelo de regresión logística para predecir la misma variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del modelo de regresión logística: 0.8505534190038458\n",
      "Matriz de confusión del modelo de regresión logística:\n",
      "[[31175  1963]\n",
      " [ 4410  5096]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Creamos una instancia del modelo de regresión logística\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Entrenamos el modelo con los datos de entrenamiento imputados\n",
    "log_reg.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_log_reg = log_reg.predict(X_test_imputed)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
    "conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f\"Exactitud del modelo de regresión logística: {accuracy_log_reg}\")\n",
    "print(\"Matriz de confusión del modelo de regresión logística:\")\n",
    "print(conf_matrix_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evalúe los dos modelos con las métricas definidas al principio. Discuta las fortalezas y debilidades de cada enfoque \n",
    "en el contexto específico de este problema comparándolos con el modelo de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Modelo Base--\n",
      "Accuracy: 0.7605759309633243\n",
      "F1-score: 0.6553882836003643\n",
      "\n",
      "--Modelo kNN--\n",
      "Accuracy: 0.846004127192571\n",
      "F1-score: 0.7368743491620866\n",
      "\n",
      "--Modelo Regresión Log.--\n",
      "Accuracy: 0.8505534190038458\n",
      "F1-score: 0.761269282544655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Modelo Base\n",
    "f1score_base = f1_score(y_test, y_pred_base, average='macro')\n",
    "\n",
    "print(\"\\n--Modelo Base--\")\n",
    "print(f\"Accuracy: {accuracy_base}\")\n",
    "print(f\"F1-score: {f1score_base}\")\n",
    "\n",
    "# Modelo kNN\n",
    "f1score_knn = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"\\n--Modelo kNN--\")\n",
    "print(f\"Accuracy: {accuracy_knn}\")\n",
    "print(f\"F1-score: {f1score_knn}\")\n",
    "\n",
    "# Modelo Reg. Log.\n",
    "f1score_log_reg = f1_score(y_test, y_pred_log_reg, average='macro')\n",
    "\n",
    "print(\"\\n--Modelo Regresión Log.--\")\n",
    "print(f\"Accuracy: {accuracy_log_reg}\")\n",
    "print(f\"F1-score: {f1score_log_reg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de Resultados\n",
    "\n",
    "| Modelo                   | Accuracy       | F1-Score          |\n",
    "|--------------------------|----------------|-------------------|\n",
    "| **Modelo Base**          | 0.7606         | 0.6554            |\n",
    "| **k-Nearest Neighbors**  | 0.8460         | 0.7369            |\n",
    "| **Regresión Logística**  | 0.8506         | 0.7613            |\n",
    "\n",
    "### Comparación y Discusión\n",
    "\n",
    "1. **k-Nearest Neighbors (kNN)**:\n",
    "   - **Fortalezas**:\n",
    "     - Mejora significativa en la **exactitud** (0.8460) y el **F1-score** (0.7369) en comparación con el modelo base.\n",
    "     - Buen rendimiento en la predicción de la clase mayoritaria (clase 0).\n",
    "   - **Debilidades**:\n",
    "     - El **recall** para la clase minoritaria (clase 1) es bajo, lo que indica que el modelo tiene problemas para identificar correctamente los casos de esta clase.\n",
    "     - Sensible a la elección de k y al balance de las clases.\n",
    "\n",
    "2. **Regresión Logística**:\n",
    "   - **Fortalezas**:\n",
    "     - Presenta la mayor **exactitud** (0.8506) y **F1-score** (0.7613) entre los modelos evaluados.\n",
    "     - Mejor equilibrio entre precisión y recall para ambas clases, lo que sugiere que maneja mejor el desbalance de clases.\n",
    "   - **Debilidades**:\n",
    "     - Aunque es el modelo con mejor rendimiento, aún presenta errores en la predicción de la clase minoritaria (clase 1), como se refleja en la matriz de confusión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Resuma los hallazgos más relevantes obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Comparación con el Modelo Base**: Ambos modelos avanzados (kNN y Regresión Logística) muestran una mejora significativa en las métricas de exactitud y F1-score en comparación con el modelo base. Esto indica que ambos enfoques ofrecen un valor añadido sustancial para el problema en cuestión.\n",
    "- **Comparación entre kNN y Regresión Logística**: La Regresión Logística supera ligeramente al kNN en ambas métricas, lo que sugiere que es el modelo más robusto en este contexto. Además, maneja mejor el desbalance de clases, lo que es crucial para la correcta clasificación de los datos minoritarios.\n",
    "\n",
    "En resumen, aunque ambos modelos avanzados mejoran el desempeño respecto al modelo base, la Regresión Logística es la opción más fuerte en este contexto, debido a su mejor rendimiento general y balance entre precisión y recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Guarde los conjuntos de entrenamiento y evaluación para el siguiente TP en archivos `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hola profesor, estos archivos no estan el github porque son muy pesados para subirlos, por lo que le enviamos tambien el zip del tp2 donde incluye estos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Guarde en formato pickle el mejor modelo, seleccionado según su criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "best_model = log_reg  \n",
    "\n",
    "# Guardamos el modelo regresión logística en formato pickle\n",
    "with open('best_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
